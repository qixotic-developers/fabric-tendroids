= Performance Test Results - Batch 15 Tendroids
:toc:
:toclevels: 3

== Test Configuration

* *Setup*: 15 cylindrical tubes in 3x5 grid
* *Geometry*: 544 vertices per tube, 8,160 total vertices
* *Animation*: Breathing wave deformation
* *Target*: 60 fps for production (15 Tendroids)

== Results Summary

=== Run 1: Cold Start

* *FPS*: 11.4 fps avg (68.40ms/frame)
* *Min/Max*: 4.0 - 16.4 fps
* *Duration*: 600 frames in 52.5s
* *Breakdown*:
** Warp deformation: 3.5ms
** USD/Fabric updates: 63ms ‚ö†Ô∏è

=== Run 2: Warmed Up

* *FPS*: 24.1 fps avg (32.04ms/frame) ‚úì *2.1x improvement!*
* *Min/Max*: 22.1 - 34.8 fps
* *Duration*: 600 frames in 24.9s
* *Breakdown*:
** Warp deformation: 1.4ms ‚úì Optimized
** USD/Fabric updates: 30ms ‚ö†Ô∏è Still bottleneck

== Analysis

=== What Warmed Up?

Between runs, USD/Fabric overhead dropped from 63ms ‚Üí 30ms:

* Shader compilation completed
* Fabric caching activated
* GPU memory warmed up
* Scene graph optimized

=== Current Bottleneck

The 30ms USD update is dominated by *Python tuple conversion*:

[source,python]
----
# Line 102-105 in fabric_batch_updater.py
fabric_vertices = list(map(lambda v: (float(v[0]), float(v[1]), float(v[2])), 
                          mesh_vertices_np))
points_attr.Set(fabric_vertices)
----

This converts 8,160 vertices from numpy ‚Üí Python tuples *every frame*.

== Optimization Paths

=== Path 1: C++ Pure Computation

*Target*: 33-40 fps (~25-30ms/frame)

Replace Python tuple conversion with C++ direct memory writes:

* Current: 1.4ms (Warp) + 30ms (Python tuples) = 31.4ms
* With C++: 0.3ms (C++) + 5-10ms (direct write) = 5-10ms
* *Expected gain*: 70-80% improvement on update path

=== Path 2: Direct GPU Memory

*Target*: 60+ fps (~16ms/frame)

Skip CPU entirely, keep everything on GPU:

* Use Fabric's direct GPU memory APIs
* Warp kernel writes directly to USD GPU buffers
* Eliminate all CPU-GPU transfers
* *Expected gain*: 100% improvement (eliminate 30ms overhead)

=== Path 3: Adaptive Updates (Fallback)

*Target*: 60 fps effective (variable update rate)

If direct GPU access unavailable:

* Distance-based LOD (far tubes update slower)
* Frustum culling (off-screen tubes don't update)
* Frame skipping (update different tubes each frame)
* *Expected gain*: Effective 60 fps for visible tubes

== Recommendations

. *‚úÖ Build C++ extension* (30 min effort, 50% improvement)
** Quick win, no architectural changes
** Gets us from 24 ‚Üí 33-40 fps
** Validates that tuple conversion is the bottleneck

. *üî¨ Investigate Fabric direct GPU APIs* (research needed)
** Could eliminate remaining 30ms entirely
** Requires deep dive into usdrt documentation
** Potential for 60+ fps

. *üìä Profile real production scene*
** Add materials, lighting, seafloor
** Measure actual frame budget
** May discover other bottlenecks (rendering, physics)

== Memory Profile

Both runs showed *stable memory*:

* No memory leaks detected
* Growth: <5MB over 600 frames
* System stable for long runs

== Next Steps

. Build C++ extension (use CLion - see BUILD_INSTRUCTIONS.md)
. Test C++ Batch 15 (should hit 33-40 fps)
. If successful, investigate Fabric GPU memory APIs
. If stuck, implement adaptive update strategy

'''

*Status*: Batch 15 baseline established at 24 fps +
*Blockers*: None - ready for C++ optimization +
*Date*: 2025-11-13
